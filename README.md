# Cancer-gene-mutation-classificationMotivation and Problem: 
Cancer treatment is one of the critical and challenging tasks in medical treatment. Cancer tumors are majorly formed due to gene mutations. The actual challenge in starting the cancer treatment is distinguishing between the mutations that majorly contribute to tumor growth. The detection of cancer tumors formed due to gene mutations plays a vital role in saving people's lives. Even after advancements in technology, pathologists do the gene mutation classification manually, which is time-consuming to interpret every genetic mutation from the clinical evidence manually. These clinical shreds of evidence belong to a total of nine classes. We can classify the gene mutations through textual evidence using Machine Learning.
The organization would be benefited if we design an efficient Machine Learning model to identify the gene mutation that is causing the tumor growth. It would be a breakthrough in mutation classification, saving a lot of time. They can start therapies to the patient early using this model than when they manually identify the gene mutation. We hope our solution would identify the gene mutation that will help detect cancer tumors effectively and quicker than the manual approach followed by pathologists. Diagnosing the cause of the disease even a short period earlier has immense significance in the medical field. The success of this approach will save numerous lives.

Solution :
We propose a multi-class classification technique to classify the genetic mutations as there are nine different classes of genes. The organization is currently working on manually classifying the gene mutation by interpreting every genetic mutation from the clinical evidence, which will be time-consuming and might have some misapprehension due to human errors. Our solution will benefit the organization in two ways. One, it reduces the manual effort of pathologists who can invest their time in practical research and treating the patients. Two, it could do better performance in classifying the gene mutation circumventing human errors. These two benefits will help the organization detect the cause of cancer and start the therapies earlier for the patients

How it works:
Dataset:
We have taken the dataset provided by an organization called Memorial Sloan Kettering Cancer Centre (MSKCC). There are two data files. The first data file is Variant and it has 4 columns.
•	ID (INT): ID is used to link the mutation to the clinical evidence. 
•	GENE (STRING): The gene where a mutation is found. 
•	VARIATION (STRING): Amino acid change for the mutation 
•	CLASS (INT): We have nine different classes ranging from 1 to 9
The second is Text file and it has 2 columns 
•	ID (INT): ID is used to link the clinical evidence to the mutation 
•	TEXT (STRING): Clinical evidence used to classify the mutation
Overview:
We are splitting the datasets as Training_variant, Testing_variant, Training_text, Testing_text. We have analyzed, cleaned, and preprocessed the dataset. Performed the Univariate Analysis on the features. Performed classification using 4 models, like Logistic Regression, KNN, Random Forest, and Navie Bayes algorithms.  Evaluated the performance of the models using Confusion matrix, precision and recall matrix, and log loss. 
Exploratory Data Analysis:
We have done the exploratory data analysis on both datasets to better understand the features of the dataset. Analyzed the number of numeric and categorical features to further process the data for the model. Visualize each feature in the data set to understand how the data is distributed, some features are equally distributed, and some gene classes are recorded in the majority compared to other genes in the dataset.  
Data Preprocessing  
We have joined the datasets using the ID field as the joining condition and checked the dataset for null values. There were five null values in the text column. As it’s medical data, we can’t drop the null values or fill them with some random values, so we have replaced the null values by concatenating Gene and Variant column values for the corresponding ID column value. 
As we have categorical features, we have changed them to numeric values for the Machine Learning model to process them. We did tokenization using nltk word_tokenize,  stop words removal using nltk corpus, and lemmatization using WordNetLemmatizer to clean the data. 
We did one-hot encoding using count vectorization on the categorical columns like gene, variant, and text. In the CountVectorizer constructor, we specify the minimum frequency of occurrence to be 3 for the text column as the text feature has lots of words while the gene and variation have only one word in it. We also normalized each word as in the case of the text feature a particular word may have a high frequency. In the case of gene and variation, most of the values were 0 and the max value was 1, so normalization was not required in those cases.
We have split the data into train, test, and cross-validation datasets for training, testing, and validating the model
Univariate Analysis:
We have three categorical features gene, variant, and text. We performed univariate analysis on each feature to analyze and identify which feature majorly influences the gene class classification. We implemented a logistic regression model with each feature as the only input and evaluated it with the log loss value in train, test, and cross-validation datasets.
Comparing the values, gene feature has better influence over the classification as there is only a minute difference between the log loss value between the train, test, and validation dataset compared to variant and text features.
Machine Learning Models:
Our problem is a multiclass classification problem so we have decided to implement four classification models. The classification has two phases, a learning phase, and an evaluation phase. In the learning phase, the classifier trains its model on a given dataset, and in the evaluation phase, it tests the classifier performance. Performance is evaluated based on various parameters such as accuracy, error, precision, and recall. 
Logistic Regression:
Since we are working on multi-class classification, logistic regression supports categorizing data into discrete classes by studying the relationship from the given set of data. Logistic regression algorithm also allows models to be updated easily to reflect new data. 
We have used the SGDClassifier function for logistic regression as an optimizer function. In our dataset, some classes occur more frequently than others. The class weight parameter is set to balanced so that it can take care of an imbalance dataset. We set the penality as l2 regularization to avoid overfitting. The loss parameter is set to log, indicating that we want to apply logistic regression. Also, we use the calibrated classifier that predicts the class probabilities rather than the absolute values and we are also using the sigmoid activation function. Predicting probabilities provides better ways to evaluate the model. 
We identified the best alpha value and predicted the class using it. We used the Log Loss (Logarithmic Loss) as a performance measurement function. In Log Loss the prediction output is a probability value between 0 and 1.

K Nearest Neighbors :
K Nearest Neighbors is a simple supervised Learning ML algorithm that can be used for classification and regression. We used the KNeighborsClassifier function for the  Model.  Also, we have used the calibrated classifier that predicts the class probabilities rather than the absolute values along with the sigmoid activation function. Predicting probabilities provides better ways to evaluate the model.  We identified the best alpha value and predicted the class using it. We used the Log Loss (Logarithmic Loss) as a performance measurement function. In Log Loss the prediction output is a probability value between 0 and 1.

Random Forest:
Random forest is a supervised learning algorithm. It is a classification algorithm that consists of several decision trees. When constructing, each particular tree in the forest makes a class prescience and the class with the maximum votes becomes the prediction of our model. The count vectors obtained from the sparse matrix are fitted and test scores are calculated by tuning the various parameters to achieve the optimum performance of the model. The average accuracy score for this model is coming out to be 49%.
We used the RandomForestClassifier function for the Model.  Also, we have used the calibrated classifier that predicts the class probabilities rather than the absolute values along with the sigmoid activation function. Predicting probabilities provides better ways to evaluate the model.  We identified the best alpha value and predicted the class using it. We used the Log Loss (Logarithmic Loss) as a performance measurement function. In Log Loss the prediction output is a probability value between 0 and 1.


Navie Bayes:
Naive Bayes is the most straightforward and fast classification algorithm, which is suitable for a large chunk of data. Naive Bayes classifiers have high accuracy and speed on large datasets. It is a statistical classification technique based on Bayes Theorem

The multinomial Naive Bayes classifier is used for classification. Also, we have used the calibrated classifier that predicts the class probabilities rather than the absolute values along with the sigmoid activation function. Predicting probabilities provides better ways to evaluate the model.  We identified the best alpha value and predicted the class using it. We used the Log Loss (Logarithmic Loss) as a performance measurement function. In Log Loss the prediction output is a probability value between 0 and 1.

Evaluation, Outcomes and Discussion:
The Performance of the models is evaluated based on various parameters such as accuracy, error, precision, and recall.  Logistic Regression has better accuracy when compared to the other models.
Logistic Regression is performing better than other models because it has got good accuracy for all the classes when compared to the other models for this data set. As the model (Logistic Regression) is predicting better thus it benefits the organization by reducing the manual classification effort and saves a lot of time. Thus, it increases the chances of saving human lives.  There could be a situation where our model may not perform well is when the clinical text is not documented properly or if it misses some important features or if the record is lost, in all these scenarios we lose important data and that affects the performance of the model.
